# mindless-machines
a project made for a science fair. Used for testing the accuracy of ChatGPT's answers when faced with questions from the MMLU.
Recent developments in Large Language Model AI have been astounding, and have been accelerating at an unprecedented rate. Thus, the questions come of how well artificial intelligence can adapt like humans can. In this paper, we present our findings for GPT-4. We asked two main types of questions: problem solving questions that require adaptation, and fact-based straightforward questions that require knowledge. We utilized an intelligence test, specifically of the type created by MENSA, as well as the Massive Multitask Language Understanding benchmark (MMLU), which is a large dataset that contains questions about many subjects at varying levels. We found that GPT-4 did abysmally on the IQ test (14.3% average), especially in comparison to the results from the MMLU test (54% average). We can conclude that Chat GPT-4 still has much more room for evolution and development until it can reach a level comparable to humans. 
